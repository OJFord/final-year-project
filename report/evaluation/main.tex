\section{Evaluation}\label{eval}

In order to evaluate the success of the project, both automated testing of the implemented system, and a more `manual' comparison to other solutions to the problem will be undertaken.

\subsection{Automated testing}\label{eval:auto-test}
The system will be evaluated in part by a suite of acceptance tests containing requests designed such that they would or would not fail at run-time; the test will pass if the compiler does or does not succeed, respectively.

These tests will be designed in such a way that each checks a particular requirement of the specification (cf.~\ref{intro:req:client-errors}) is met. For example,  in order to test that use of an incompatible HTTP method is prohibited, one test will attempt to compile a program that uses a method not offered by the test schema, and verify that the error is as expected.

At least some of the tests will be designed relatively early on in the project (cf.~\ref{task-create-test-suite}) in order to aid implementation and avoid regression.

\subsection{Solutions compared}\label{soln:comparison}
Section~\ref{soln} describes some existing solutions to the problem (cf.~\ref{intro:motivation}) and why there is still a problem left for this project to solve (cf.~\ref{soln:summary}).

Part of the evaluation will therefore re-examine the existing solutions, and how they perform for a small (though aiming to reflect real-world usage) example application relative to the solution to come out of this project.

This evaluation will document the types of error it is possible to obtain in the same example application under the different solutions, as well as their relative ease of use or disruption to the development process, inline with the `soft' requirements (cf.~\ref{intro:req:soft}).
