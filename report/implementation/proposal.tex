\subsection{Proposal} \label{impl:proposal}

\emph{gRPC} and \emph{Cap'n Proto} in particular are elegant existing solutions to the problem addressed by this project. The issue with them is largely one of adoption: the tight-coupling that makes the frameworks so effective for use with organisations' internal APIs is also limiting their use in external APIs.

Therefore the proposal here is to bring the type-safety of making requests via stubs generated from a protocol definition to the wealth of existing REST APIs. These APIs offer actions on resources; the solution presented here will validate that a client interacts properly with these resources, at least according to the API's schema.

The Open API Specification (cf.~\cref{bg:OAS}) will be used as the dialect of schema against which to validate client device programs. OAS is chosen as it is more expressive than JSON Hyper-Schema, which lacks a description of response types, required authentication, or other features found in the former. Knowing the response types will allow typing the result as used by the client, and knowledge of which endpoints require authentication will be necessary for a useful analysis of a multi-request session. OAS (and presently Swagger) also appears to be much more widely used than Hyper-Schema~\cite{anyone_use_hyperschema} with an existing rich ecosystem of open-source validators and stub generators~\cite{swagger_oss}. \label{impl:proposal:schema}

Rust is targeted as the language of client device programs, as it is something of an `emerging market' for embedded software, and offers a much richer type system than C or C++ (which would be the viable alternatives) that we can leverage to describe a REST API. \label{impl:proposal:lang}

Given a schema for correct consumption of an API, there are three models for how we might go about validation:
\begin{enumerate}
	\item Wait for type-checking to complete; then lint the HIR (cf.~\cref{bg:rust:ir}) for HTTP requests, and match constant URL strings against a schema verifying the generated type tables against the types therein.
	\item With a schema file as user input, infer a set of functions, types, and traits that represent valid use of the API; then let the compiler work as normal to match user code against the known-valid types.
	\item Extend the syntax (cf.~\cref{bg:rust:plugins}) to admit a new form of function representing a request; then plug in to the compiler before type-checking (or add a pass to the existing checker) to unify types against the schema, used as an additional context set on the left of the derivation (cf.~\cref{bg:types:lambda-calculus}).
\end{enumerate} \label{impl:proposal:validation-models}

Validation that relies on knowledge of prior requests (cf.~\cref{intro:req:sessions}) will certainly be easier to implement - if not possible only - under the first model with a compiler plugin (cf.~\cref{bg:rust:plugins}) that is able to walk the CFG and inspect the path between requests and their respective dependants.

The second model would however afford a welcome usability of the response body, as the type returned by the request function could capture the structure of the data contained within the server's response. This would be of particular use if a developer used the returned data in a further request: under the first model we would only have type information for the structure if they had chosen to deserialise and re-serialise the data, which may even have been deliberately avoided  for reasons of performance. Providing the user with this type would ensure that it exists when used as an input to the second request, and could be achieved without overhead since the structure is known from the schema. Further, with reference to the stated requirement for productivity in \cref{intro:req:productivity}, it would be of great help to provide the user with a natural interface to the types returned - \mintinline{rust}{response.body.products[2].price} for example - not least because of the assistance this allows tools such as auto-complete to provide; that they could not given a generic object indexed by strings such as \mintinline{rust}{"price"}.

A further complication of conducting the entire analysis at the MIR stage is due to there being no single or standard implementation of an HTTP client for Rust. (cf.~\cref{bg:rust:http}) We would therefore need either to look for specific third-party function identifiers, each subject to change and none other supported, or attempt to match on the system calls to which each library must inevitably reduce. The latter method would likely still require third-party specific restructuring of the arguments and request body in order to type-check them against the schema, and some information may be lost.

The third model would be appealing to reason about, but in practice would likely offer nothing but additional complexity. From the perspective of user-productivity, it would not have the benefit from tooling seen by the second, and would worsen the situation compared to the first model: compilation errors would be cryptic and confusing; bugs in user code indistinguishable from bugs stemming from the validation library itself.

Thus, functions and types for interacting with an API will be inferred from its schema via a macro, and dependencies between requests - most prominently \code{412 Precondition Failed} requiring single-step progression of the resource as a state machine, and \code{424 Failed Dependency} in which creation/deletion of a dependant object must follow that of its dependency - will be validated by a late-stage (that is, on the HIR tree and with access to type-check tables) lint pass. \cfref{bg:rust:plugins} \label{impl:proposal:validation}