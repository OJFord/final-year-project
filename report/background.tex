The background study for this project consists of other solutions to this or similar problems - such as Remote Procedure Calls, service discovery, and API discovery - as well as literature relevant and useful to the proposed solution that follows.

\subsection{Existing solutions}\label{existing-solutions}
\subsubsection{Remote Procedure Calls}\label{rpc}
RPC is a generic term for protocols implementing remote invocation of procedures from a separate process. The server (that machine which hosts the procedure) and client (which does the invoking) are often separated by a network, but RPC is also used inter-process on a single machine to structure communication over Unix sockets \cite{rpc_unix_sockets}.

RPC differs from REST (or `standard' HTTP\footnote{RPC still needs a transport mechanism, and can and has been used over HTTP such as in xml-rpc \cite{xml-rpc} and SOAP \cite{ietf-soap-draft}.}) not just in being a different protocol (strictly speaking, a set of protocols in the case of RPC) but so too in the manner in which the server-side code is executed. Whereas HTTP is a request for a resource, stemming from its origins in more FTP-like document-sharing \cite{http_history, http_vs_ftp}, RPC is a much tighter coupling - more akin to calling a library function, only the library happens to be dynamically linked over IP.

Client-server interactions in which both entities are controlled (or maintained) by a single party are well-suited to RPC, but this is often not the case in an IoT context, where it is common for a client device manufacturer to be using another vendor's back-end server.

Remote Procedure Calls could certainly be made over HTTP, CoAP, or even a queuing transport such as MQTT \cite{mqtt-rpc} - but this is typically not the case; more often we see REST-ful HTTP or CoAP APIs, and domain-specific data structures passed through message queues, giving a more loosely-coupled interaction than RPC affords.

\subsubsection{Service discovery}\label{service-discovery}
Service discovery mechanisms are a common pattern in Service-Oriented (or micro-service) Architectures (SOA) that allow dynamically managed (usually virtually hosted) services to `discover' one another like a plumber looking up an electrician in the \emph{Yellow Pages}. 

These are useful for at least delivering a request to the correct place, but typically do not offer anything further by way of validation.

Linkerd \cite{service_discovery_linkerd}, Consul \cite{service_discovery_consul}, and Apache Zookeeper \cite{service_discovery_apache} are examples of such mechanisms.

\subsubsection{API discovery}\label{api-discovery}
API discovery, that is, services or tools facilitating the discovery of the specific details of an API (in contrast to service discovery, which is more of a micro-DNS) can generally be categorised as one of two types:

\begin{enumerate}
\item Those that provide a registry of user-provided information, intended largely for human consumption, and motivated by driving users to the API;
\item Those that \emph{generate} a schema for the API based on its source language, or a secondary configuration.
\end{enumerate}

As we are interested in automating as many validations as possible, exactly to lessen the need for human intervention, only the second type is of interest to this project.

Swagger \cite{swagger} is a popular example; defining a specification for a YAML \cite{yaml} configuration of an API, with community traction in implementing inference engines for many languages and API framework DSLs \cite{swagger_oss}. Its focus is primarily on the server-side, using the schema to generate syntactically-correct documentation. (cf.~\ref{OAS})

\subsubsection{Summary}\label{existing-soln-summary}
RPCs provide a tight-coupling that works excellently when the same user (possibly a team of software engineers) has autonomy over both client and server.

Swagger, by contrast, is essentially zero-coupling, but provides a mechanism for an API vendor to maintain synchrony between their documentation and code. Some client stub generators exist, to some extent `forcing' the user to use correct methods and paths. These seem only to exist for high-level languages, though, and without fully matching the request body to an ADT in the client source language. \cite{swagger_oss}

Thus, there seems to be space for a solution that ties client and server more tightly than existing Swagger tooling, but maintaining the REST resource-access pattern distinct from RPC.

This would allow the solution to meet the specified `soft' requirements (cf.~\ref{soft-reqs}) of not disrupting existing work-flow, while allowing greater scope for detecting errors in client usage of an API, as compared to the referenced server's expectation.

\subsection{Related works}

\subsubsection{$\lambda$-calculus}\label{lambda-calculus}
The untyped lambda calculus was introduced by Church in 1936; the set of terms $\Lambda$ is ranged over by $M$ as: $$
M \Coloneqq x \mid (\lambda{x}.M') \mid (M' \cdot M'')
$$ where $x$ is a variable; with parentheses and application operator ($\cdot$) often omitted. \cite{tsfpl}

\begin{defn}$\alpha$-conversion\label{def:alpha-conversion}
	$$
	\lambda{x}.M =_{\alpha} \lambda{y}.(M[y/x]) \ \ \ (y\notin{M})
	$$
\end{defn}

That is, terms differing only in the names of bound variables are ($\alpha$-)equivalent.

\begin{defn}Term substitution\label{def:term-substitution}
	$$
	\begin{aligned}
		x[N/x] &= N \\
		y[N/x] &= y &(y \neq x) \\
		(\lambda{y}.M) &= \lambda{y}.(M[N/x]) \\
		(PQ)[N/x] &= P[N/x]Q[N/x]
	\end{aligned}
	$$
\end{defn}

\begin{defn}$\beta$-reduction\label{def:beta-conversion}
	$$
	\begin{aligned}
		(\lambda{x}.M)N &\to_{\beta} M[N/x] \\
		M \to_{\beta} N &\Rightarrow \begin{cases}
 			\lambda{x}.M \to_{\beta} \lambda{x}.N \\
 			PM \to_{\beta} PN \\
 			MP \to_{\beta} NP
 		\end{cases}
	\end{aligned}
	$$
\end{defn}

$\beta$-conversion is a binary relation on $\langle{M,N}\rangle$, where $M$ is a \emph{reducible expression} (i.e. has the form $PQ$ where $P$ is an abstraction) and $N$ is its \emph{contractum}, the result of applying term substitution to $M$. The `many-step' relation $\twoheadrightarrow_{\beta}$ is its reflexive and transitive closure. \cite{tsfpl}

\begin{defn}$\eta$-reduction\label{def:eta-reduction}
	$$
	\begin{aligned}
	\lambda{x}.Mx &\to_{\eta} M &(x\notin{fv(M)})
	\end{aligned}
	$$
\end{defn} where $fv(M)$ denotes the free variables of $M$.

\paragraph{Extending $\Lambda$:}\label{extending-lambda}

$\Lambda$ is often used as a basis for reasoning about programming languages, on which more and higher-level features can be built \cite{tpl}. For example, adding explicit substitution and garbage collection: $$
M \Coloneqq \dots \mid x\langle{x \coloneqq N}\rangle
$$ then, similarly to $\beta$-reduction, the one-step and contextual closure: \cite{lambda_xgc, tsfpl}
$$
\begin{aligned}
	&(\textrm{xv})\colon &x\langle{x \coloneqq N}\rangle &\to_{\textrm{xgc}} N \\
	&(\textrm{xvgc})\colon &x\langle{y \coloneqq N}\rangle &\to_{\textrm{xgc}} x &(y \neq x) \\
	&(\textrm{xab})\colon &(\lambda{x}.M)\langle{y \coloneqq N}\rangle &\to_{\textrm{xgc}} \lambda{x}.(M\langle{y \coloneqq N}\rangle) \\
	&(\textrm{xap})\colon &(MM')\langle{y \coloneqq N}\rangle &\to_{\textrm{xgc}} (M\langle{y \coloneqq N}\rangle)(M'\langle{y \coloneqq N}\rangle) \\
	&(\textrm{gc})\colon &M\langle{x \coloneqq N}\rangle &\to_{\textrm{xgc}} M &\big(x \notin fv(M)\big)
\end{aligned}
$$$$
\begin{aligned}
	M \to_{\textrm{xgc}} N \Rightarrow \begin{cases}
		ML &\to_{\textrm{xgc}} NL \\
		LM &\to_{\textrm{xgc}} LN \\
		\lambda{x}.M &\to_{\textrm{xgc}} \lambda{x}.N \\
		M\langle{x \Coloneqq L}\rangle &\to_{\textrm{xgc}} N\langle{x \Coloneqq L}\rangle \\
		L\langle{x \Coloneqq M}\rangle &\to_{\textrm{xgc}} L\langle{x \Coloneqq N}\rangle
	\end{cases}
\end{aligned}
$$

\subsubsection{Curry type assignment}\label{curry}
The set of types $\mathcal{T}_C$ is ranged over by $A$ as: $$
A \Coloneqq \varphi \mid (A \to B)
$$ where $\varphi$ ranges over the set of type variables $\Phi$. \cite{tsfpl}

A \emph{statement} is of the form $M\colon{A}$, where $M$ is the subject and $A$ the predicate. A set of statements forms the context $\Gamma$, for which $x\in{\Gamma}$ iff $\exists A$ such that $x\colon{A}\in\Gamma$.

\begin{defn}Curry type assignment - derivations\label{def:curry-derivations}
	$$
	\begin{aligned}
		&(\textrm{Ax}): \Inf{
			\Gamma,x\colon{A} \vdash_C x\colon{A}
		} \\
		\\
		&(\to\textrm{I}): \Inf[x\notin{\Gamma}]{
			\Gamma,x\colon{A} \vdash_C M\colon{B}
		}{
			\Gamma \vdash_C \lambda{x}.M\colon{A\to{B}}
		} \\
		\\
		&(\to\textrm{E}): \Inf{
			\Gamma \vdash_C M\colon{A\to{B}} \ \ \ \Gamma \vdash_C M'\colon{A}
		}{
			\Gamma \vdash_C MM'\colon{B}
		}
	\end{aligned}
	$$
\end{defn}
Where $\Gamma,x\colon{A}$ is written to mean $\Gamma\cup\{x\colon{A}\}$ where either $x\colon{A}\in\Gamma$ or $x\notin\Gamma$.

\begin{defn}Subject reduction\label{def:subject-reduction}
	$$
	(\Gamma \vdash_C M\colon{A}) \& (M \twoheadrightarrow_{\beta} N)
	\ \Longrightarrow \
	\Gamma \vdash_C N\colon{A}
	$$
\end{defn}

\begin{defn}Type substitution $S = (\varphi\mapsto{C})\colon{\mathcal{T}_C\to\mathcal{T}_C}$\label{def:type-substitution}
	$$
	\begin{aligned}
		&(\varphi\mapsto{C})\varphi &= \ &C
		\\
		&(\varphi\mapsto{C})\varphi' &= \ &\varphi' \ &\ (\varphi'\neq\varphi)
		\\
		&(\varphi\mapsto{C})A\to{B} &= \ &((\varphi\mapsto{C})A)\to((\varphi\mapsto{C})B)
	\end{aligned}
	$$$$
	\begin{aligned}
		S \circ S' &= S(S'A) \\
		S\Gamma &= \{x\colon{SB} \mid x\colon{B}\in\Gamma\}
	\end{aligned}
	$$
\end{defn}
Type substitution is provably sound and complete, by induction on the structure of derivations (cf. Definition~\ref{def:curry-derivations}). \cite{tsfpl}

As in the untyped $\Lambda$, Curry's system gives a good basis on which to build further abstraction, such as Boolean or variant types. \cite{tpl}

\subsubsection{Open API Specification}\label{OAS}

The `Open API Specification' (OAS) is a work-in-progress evolution of the proprietary Swagger specifications; volunteered by the company to form the \emph{Open API Initiative} (OAI) which aims to provide `a vendor neutral API description format'. \cite{about_oai}

The specification consists of multiple `objects' describing security, available paths, possible responses, the structure of requests, etc. - it is the `schema object' for the structure of request bodies that will be of most interest.

\begin{codelisting}{code:oas-schema-obj}{Example of an OAS Schema Object}
\begin{minted}{yaml}
type: object
required:
- name
properties:
  name:
    type: string
  address:
    $ref: '#/definitions/Address'
  age:
    type: integer
    format: int32
    minimum: 0
\end{minted}
\end{codelisting}

For example, listing~\ref{code:oas-schema-obj} gives an OAS schema object that describes an object with at least a \code{name} field, and optionally an \code{address} and \code{age}. It also specifies that \code{name} is of type \code{string}, \code{age} is an unsigned $32$-bit integer, and the type of \code{address} is defined elsewhere in the document (in the `definitions object') to enable reuse.

\begin{codelisting}{code:oas-path-obj}{Example of an OAS Path Object}
\begin{minted}{yaml}
/addresses:
  summary: Address book endpoint
  get:
    description: List addresses in user's address book
    responses:
      200:
        description: List of addresses
        content:
          application/json:
            schema:
              type: array
              items:
                $ref: '#/definitions/Address' 
  post:
    description: Add a new address
    requestBody:
      $ref: '#/definitions/Address'
    responses:
      405:
        description: Not implemented
/addresses/{address-id}:
  summary: Individual address
  get:
    description: Fetch a single address
    parameters:
    - name: address-id
      in: path
      description: ID of address
      required: true
      type: string
    responses:
      200:
        description: Address
        content:
          application/json:
            schema:
              $ref: '#/definitions/Address'
\end{minted}
\end{codelisting}

A path object, such as the two top-level entities in listing~\ref{code:oas-path-obj}, specifies the URI at which a particular endpoint is available, along with the HTTP methods that it accepts. For each endpoint, it then gives a description of the structure of expected requests, and that of the server's response.

An equivalent JSON document format is also offered, better demonstrating that the `schema object' in the OAS implements a subset of JSON Schema \cite{oas_v3} - the JSON document structural validation language \cite{json_schema} - and that the document itself is describable as a JSON Schema \cite{oas_json_schema}.

JSON Schema also defines an extension - `Hyper-Schema' - that is similar in nature to the OAS, though strictly JSON, with different syntax, and restricted in scope. \cite{leach_elegant_apis_2014}

A third alternative, `RAML', is similar in feature set to the OAS but with a syntax differing enough to break some JSON Schema compatibility. \cite{raml_v1} OAS also has the benefit of greater community use (inherited from Swagger), and the advantage that comes from an open source initiative aiming for industry standard, as opposed to semi-open proprietary standard.

\subsection{Rust}
\subsubsection{Language}\label{rust-lang}
The Rust programming language is designed for `[memory] safety, speed, and concurrency', achieved through `many zero-cost abstractions'. These include `ownership', which ensures exactly one binding for a resource; scoped `borrowing' (from the `owner') of references, where at any one time there are either immutable references borrowed, or a single mutable reference - making data races impossible - and `lifetimes' which ensure a borrowed reference is not used after having been freed. \cite{rust_book}

The language is statically typed, offers algebraic data types and their de-structuring via pattern matching; \cite{rust_match_mut_move} static dispatch with `trait bounds' to limit genericness of functions (`monomorphised' at compile-time) \cite{rust_book} and dynamic dispatch via `trait objects', analogous to type classes in other languages, such as Haskell \cite{rust_functional}. \cite{rust_type_system}

\begin{codelisting}{code:trait-bounds}{An example trait bounding in Rust. \mintinline{rust}{introduce_dog} will be `monomorphised': instantiated for the concrete type \mintinline{rust}{IrishTerrier} to enable static dispatch.}
\begin{minted}{rust}
	trait IsDog {
		fn breed(&self) -> str;
		fn name(&self) -> str;
	}

	struct IrishTerrier {
		name: str;
	}

	impl IsDog for IrishTerrier {
		fn breed(&self) -> str {
			"Irish Terrier"
		}

		fn name(&self) -> str {
			self.name
		}
	}

	fn introduce_dog<T: IsDog>(dog: T) {
		println!("This is {}, an {}", dog.name(), dog.breed());
	}
\end{minted}
\end{codelisting}

Rust does not have a garbage collector, making it possible to use in an embedded context, with several MIPS and ARM targets supported \cite{rust_platforms}. \emph{zinc}, \cite{rust_rtos_zinc} for example, is a `bare-metal' RTOS for ARM that is implemented without any C code\footnote{Many projects cross-compiling Rust for, say, ARM Linux will use Rust's FFI to call device drivers in C \cite{rust_baremetal}} and aiming for as little assembly as possible.

\subsubsection{HTTP}\label{rust-http}
Like other systems programming languages, Rust's standard library does not include an HTTP client.

\emph{hyper} is a third-party HTTP library for Rust, which offers `a low-level typesafe abstraction over raw HTTP, providing an elegant layer over ``stringly-typed'' HTTP'. The client provides typed access to the response status code, headers, and implements the \code{Read} trait for access to the response body. \cite{rust_http_hyper}

Other HTTP client implementations include the less popular \emph{solicit} \cite{rust_http_solicit} which implements \code{HTTP/2}, and \emph{Teepee} - on which development has `stalled' \cite{rust_http_teepee}.

\subsubsection{Macro rules, syntax extensions, and compiler plugins}\label{rust-plugins}

Macros in Rust can be either declarative, as in listing~\ref{code:declarative-macro}, or procedural. Declarative macros (`macros by example') match a pattern in their input, and emit the code in the corresponding block \cite{rust_macros_overview}. Procedural macros are further divided into `syntax extensions' and `compiler plugins', each taking a \mintinline{rust}{TokenStream} as input, and returning another. \cite{rust_macros_whereweat}

\begin{codelisting}{code:declarative-macro}{An example of a declarative macro in Rust}
\begin{minted}{rust}
	// print many lines by: printlns!["Line 1", "Line 2"]
	macro_rules! printlns {
		( $( $x:expr ),* ) => {
			$(
				println!($x);
			)*
		};
	}
\end{minted}	
\end{codelisting}

The \mintinline{rust}{TokenStream} argument represents the AST node on which the plugin's name is annotated, or its argument if used inline. It can be modified - or discarded - and a different \mintinline{rust}{TokenStream} returned to be compiled in its place. A popular example \cite{rust_macros_overview, rust_book} is providing Roman numerals as an integer representation: \mint{rust}|let five: u8 = 26 - roman!(XI)| a `normal' function could only offer this functionality if the argument \mintinline{rust}{XI} were given as a string, \mintinline{rust}{"XI"}.

Syntax extensions in node-annotation form (\mintinline{rust}{#[foo]}) can also have attributes, in which case they receive an additional \mintinline{rust}{Option<TokenStream>} argument, which contains all the tokens between the (optional) attribute delimiters \mintinline{rust}{()} - for example, \mintinline{rust}{#[foo(a, b)]} would call \mintinline{rust}{foo} with \mintinline{rust}{Some([Ident(a), Comma, Ident(b)])} in addition to the token stream representing the annotated AST node. \cite{rust_rfc1566}

\emph{Diesel}, a SQL database ORM and query builder, uses syntax extensions to infer a model for a database from its `information schema', given a URL. \cite{rust_orm_diesel}

Compiler plugins, the other form of procedural macro\footnote{Some references \cite{rust_macros_overview} use `compiler plugin' to refer more generally to procedural macros, and leave the set difference from syntax extensions unnamed.}, are visitor-pattern objects implementing the appropriate traits for the compiler interface to which they attach. Interfaces are available for each compiler stage, from early-stage lint passes down to naming an LLVM pass, which is registered from a C++ shared object. \cite{rust_macro_registry, rust_tests_llvm_pass}

\subsubsection{Mid-level IR}\label{rust-mir}
The Rust compiler's `mid-level IR' (MIR), as it's name suggests, is a stage part of the way through compilation; after what Rust terms the `high-level IR' (HIR) - `roughly an abstract syntax tree' - and before reaching LLVM IR. \cite{rust_mir}

MIR code exists within basic blocks, each containing a list of statements and interconnected by edges representing branch paths to form a control-flow graph (CFG). The terminators (defining which branch should be followed) are still based on Rust primitives such as \mintinline{rust}{Some}, \mintinline{rust}{None}, and pattern matching, rather than the lower level Boolean branching of LLVM IR. \cite{rust_mir}

The MIR CFG is formed after the type-checker has inferred and validated types on the HIR tree. MIR still contains reference information, as the borrow-checker runs before MIR is optimised and transformed into LLVM code. \cite{rust_mir}

Compiler plugins have access to the MIR CFG via the `plugin registry', with \mintinline{rust}{rustc_plugin::Registry.register_mir_pass}, which registers a visitor implementing the \mintinline{rust}{MirPass} trait to inspect or modify the CFG during compilation. \emph{rustproof} uses this technique to generate verification conditions under predicate transformer semantics for functions tagged with pre- and post-conditions, before checking them with an SMT solver. \cite{rust_rustproof}
